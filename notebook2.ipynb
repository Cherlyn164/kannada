{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":16017,"databundleVersionId":708189,"sourceType":"competition"},{"sourceId":8610812,"sourceType":"datasetVersion","datasetId":5152991}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# 数据读取和预处理\ntrain = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\ntest = pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')\n\n# 将训练数据划分为训练集和验证集\ntrain_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n\nprint(\"训练数据集:\", train_df.shape)\nprint(\"验证数据集:\", val_df.shape)\nprint(\"测试数据集:\", test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T15:08:49.718907Z","iopub.execute_input":"2024-06-26T15:08:49.719870Z","iopub.status.idle":"2024-06-26T15:09:01.771588Z","shell.execute_reply.started":"2024-06-26T15:08:49.719832Z","shell.execute_reply":"2024-06-26T15:09:01.770429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\n\n# 从DataFrame中提取图像像素和标签\nimages = train.iloc[:, 1:].values  \nlabels = train.iloc[:, 0].values   \n\n# 将图像从平铺的数组转换成28x28的矩阵\nimages = images.reshape(-1, 28, 28)\n\n# 绘制前16张图像\nplt.figure(figsize=(8, 8))\nfor i in range(16):\n    plt.subplot(4, 4, i + 1)  # (4 rows, 4 columns)\n    plt.imshow(images[i], cmap='gray')  # 显示灰度图像\n    plt.title(f'Label: {labels[i]}')  # 设置标题显示标签\n    plt.axis('off')  \nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KannadaMNISTDataset(Dataset):\n    def __init__(self, dataframe, transform=None, is_test=False):\n        self.data = dataframe\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = np.array(self.data.iloc[idx, 1:], dtype=np.uint8).reshape((28, 28, 1))\n        if self.transform:\n            image = self.transform(image)\n        if self.is_test:\n            return image\n        label = self.data.iloc[idx, 0]\n        return image, label\n\n# 数据加载器\ndef get_data_loaders(train_df, val_df, test_df, batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n    train_dataset = KannadaMNISTDataset(train_df, transform=transform)\n    val_dataset = KannadaMNISTDataset(val_df, transform=transform)\n    test_dataset = KannadaMNISTDataset(test_df, transform=transform, is_test=True)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, test_loader\n\nbatch_size = 64\ntrain_loader, val_loader, test_loader = get_data_loaders(train_df, val_df, test, batch_size)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LeNet模型定义\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = LeNet()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练函数\ndef Ktrain(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n    model.train()\n    train_losses = []\n    val_losses = []\n    for epoch in range(num_epochs):\n        running_train_loss = 0.0\n        running_val_loss = 0.0\n        \n        # 训练阶段\n        for images, labels in train_loader:  # 遍历训练数据集中的每一批图像和标签\n            optimizer.zero_grad()  # 清除上一轮的梯度\n            outputs = model(images)  # 将图像输入模型，获得输出结果\n            loss = criterion(outputs, labels)  # 计算模型输出与实际标签之间的损失\n            loss.backward()  # 反向传播，计算梯度\n            optimizer.step()  # 根据梯度更新模型参数\n            running_train_loss += loss.item()  # 累加当前批次的训练损失\n        \n        # 验证阶段\n        model.eval()  # 将模型设置为评估模式，禁用dropout等正则化方法\n        with torch.no_grad():  # 禁用梯度计算，以减少内存消耗\n            for images, labels in val_loader:  # 遍历验证数据集中的每一批图像和标签\n                outputs = model(images)  # 将图像输入模型，获得输出结果\n                loss = criterion(outputs, labels)  # 计算模型输出与实际标签之间的损失\n                running_val_loss += loss.item()  # 累加当前批次的验证损失\n\n        model.train()  # 将模型设置回训练模式\n        \n        average_train_loss = running_train_loss / len(train_loader)\n        average_val_loss = running_val_loss / len(val_loader)\n        train_losses.append(average_train_loss)\n        val_losses.append(average_val_loss)\n        print(f\"Epoch {epoch + 1}, Train Loss: {average_train_loss}, Val Loss: {average_val_loss}\")\n    \n    return train_losses, val_losses\n\n# 定义损失函数\ncriterion = nn.CrossEntropyLoss()  # 使用交叉熵损失函数，适用于多分类任务\n# 定义优化器\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # 使用随机梯度下降（SGD）优化器，学习率为0.001，动量为0.9\nnum_epochs = 50  # 训练50个周期\n\ntrain_losses, val_losses = Ktrain(model, criterion, optimizer, train_loader, val_loader, num_epochs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 可视化训练和验证损失\ndef plot_losses(train_losses, val_losses):\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.show()\n\nplot_losses(train_losses, val_losses)\n\n# 模型评估\ndef evaluate_model(model, val_loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.numpy())\n            y_pred.extend(predicted.numpy())\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    cm = confusion_matrix(y_true, y_pred)\n    return accuracy, precision, recall, f1, cm\n\naccuracy, precision, recall, f1, cm = evaluate_model(model, val_loader)\nprint(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\nprint(\"Confusion Matrix:\")\nprint(cm)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 生成提交文件\ndef generate_submission_file(model, test_loader, filename=\"submission.csv\"):\n    model.eval()\n    results = []\n    with torch.no_grad():\n        for images in test_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            results.extend(predicted.numpy())\n    submission_df = pd.DataFrame({\"id\": np.arange(len(results)), \"label\": results})\n    submission_df.to_csv(filename, index=False)\n\ngenerate_submission_file(model, test_loader)\n\n# 检查生成的提交文件\n#submission = pd.read_csv(\"E:/b/submission.csv\")\n#print(submission.head())\n#print(\"提交文件行数:\", len(submission))\n#print(\"测试数据集行数:\", len(test))\n\n# 确认提交文件行数与测试数据集行数相同\n#assert len(submission) == len(test), \"提交文件的行数应与测试数据集的行数相同\"\n","metadata":{},"execution_count":null,"outputs":[]}]}